# ğŸ¥ Advanced Video Event Detection & Extraction

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.25%2B-FF4B4B.svg)](https://streamlit.io)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100%2B-009688.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://docker.com)

A comprehensive AI-powered video analysis platform that combines multiple state-of-the-art computer vision models to detect, analyze, and extract events from video content using natural language queries.

## ğŸš€ Key Features

### ğŸ¯ Core Capabilities
- **Natural Language Video Queries** - Search videos using text descriptions
- **Enhanced Person Detection** - Advanced person recognition with background independence
- **Image Matching & Cross-Domain Vision** - Find similar objects across different conditions
- **Small Object Detection** - Specialized detection for tiny objects in videos
- **Multi-Modal AI Integration** - OpenCLIP, BLIP-2, MediaPipe, and YOLOv8
- **Real-time Processing** - Streamlit web interface with FastAPI backend

### ğŸ”¬ Advanced Features
- **Background Independence** - Detect objects regardless of background changes
- **Cross-Domain Matching** - Match objects across color/grayscale differences
- **Adaptive Thresholding** - Dynamic confidence adjustment based on context
- **Region Proposal Networks** - Enhanced small object detection
- **Performance Monitoring** - Real-time system performance tracking
- **Memory Management** - Intelligent resource optimization

## ğŸ› ï¸ Tech Stack

### Core Technologies
- **Python 3.8+** - Primary programming language
- **PyTorch 2.0+** - Deep learning framework
- **Transformers** - Hugging Face model library
- **OpenCLIP** - Contrastive language-image pre-training
- **MediaPipe** - Real-time perception pipeline
- **OpenCV** - Computer vision library

### Web Framework
- **Streamlit** - Interactive web interface
- **FastAPI** - High-performance API backend
- **Uvicorn** - ASGI server

### AI Models
- **OpenCLIP** - Vision-language understanding
- **BLIP-2** - Bootstrapped vision-language pre-training
- **YOLOv8** - Object detection
- **MediaPipe** - Pose and face detection

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8 or higher
- Git
- 8GB+ RAM recommended
- GPU support optional but recommended

### Local Installation

1. **Clone the repository**
```bash
git clone https://github.com/nb-hmd/Advanced-Video-Event-Detection-Extraction.git
cd Advanced-Video-Event-Detection-Extraction
```

2. **Create virtual environment**
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Download models** (Optional - models will be downloaded automatically on first use)
```bash
# YOLOv8 model is included
# Other models will be downloaded from Hugging Face automatically
```

### Docker Installation

1. **Using Docker Compose** (Recommended)
```bash
docker-compose up --build
```

2. **Using Dockerfile**
```bash
docker build -t video-event-detection .
docker run -p 8501:8501 -p 8000:8000 video-event-detection
```

## ğŸš€ Usage

### Web Interface (Streamlit)

1. **Start the application**
```bash
# Using the robust server (recommended)
python robust_server.py

# Or directly with Streamlit
streamlit run src/web/streamlit_app.py --server.port 8501
```

2. **Access the web interface**
   - Open your browser and navigate to `http://localhost:8501`
   - Upload a video file
   - Choose your analysis type:
     - Natural language queries
     - Person detection
     - Image matching
     - Small object detection

### API Usage (FastAPI)

1. **Start the API server**
```bash
uvicorn src.api.main:app --host 0.0.0.0 --port 8000
```

2. **API Documentation**
   - Interactive docs: `http://localhost:8000/docs`
   - OpenAPI spec: `http://localhost:8000/openapi.json`

### Example API Calls

#### Upload Video
```python
import requests

with open('video.mp4', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/api/upload',
        files={'file': f}
    )
video_id = response.json()['video_id']
```

#### Natural Language Query
```python
response = requests.post(
    'http://localhost:8000/api/query',
    json={
        'video_id': video_id,
        'query': 'person walking with a dog',
        'mode': 'mvp',
        'top_k': 5
    }
)
results = response.json()['results']
```

#### Enhanced Person Detection
```python
response = requests.post(
    'http://localhost:8000/api/enhanced-person-detection',
    json={
        'video_id': video_id,
        'reference_image_path': 'person.jpg',
        'top_k': 10
    }
)
matches = response.json()['matches']
```

## ğŸ“ Project Structure

```
Advanced-Video-Event-Detection-Extraction/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                    # FastAPI backend
â”‚   â”‚   â””â”€â”€ main.py            # API endpoints
â”‚   â”œâ”€â”€ models/                # AI model implementations
â”‚   â”‚   â”œâ”€â”€ openclip_model.py  # OpenCLIP integration
â”‚   â”‚   â”œâ”€â”€ blip_model.py      # BLIP-2 integration
â”‚   â”‚   â””â”€â”€ univtg_model.py    # UniVTG integration
â”‚   â”œâ”€â”€ services/              # Core business logic
â”‚   â”‚   â”œâ”€â”€ video_processor.py # Main video processing
â”‚   â”‚   â”œâ”€â”€ enhanced_person_detector.py
â”‚   â”‚   â”œâ”€â”€ image_matcher.py   # Image matching algorithms
â”‚   â”‚   â”œâ”€â”€ small_object_detector.py
â”‚   â”‚   â””â”€â”€ universal_detector.py
â”‚   â”œâ”€â”€ pipeline/              # Processing pipelines
â”‚   â”‚   â”œâ”€â”€ phase1_mvp.py      # Basic processing
â”‚   â”‚   â”œâ”€â”€ phase2_reranker.py # Result reranking
â”‚   â”‚   â””â”€â”€ phase3_advanced.py # Advanced processing
â”‚   â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”‚   â”œâ”€â”€ logger.py          # Logging utilities
â”‚   â”‚   â”œâ”€â”€ memory_manager.py  # Memory optimization
â”‚   â”‚   â””â”€â”€ model_cache.py     # Model caching
â”‚   â””â”€â”€ web/                   # Web interface
â”‚       â””â”€â”€ streamlit_app.py   # Streamlit application
â”œâ”€â”€ data/                      # Data directories
â”‚   â”œâ”€â”€ videos/               # Uploaded videos
â”‚   â”œâ”€â”€ clips/                # Extracted clips
â”‚   â”œâ”€â”€ frames/               # Extracted frames
â”‚   â””â”€â”€ embeddings/           # Cached embeddings
â”œâ”€â”€ models/                   # Model storage
â”‚   â”œâ”€â”€ openclip/            # OpenCLIP models
â”‚   â”œâ”€â”€ blip/                # BLIP models
â”‚   â””â”€â”€ yolo/                # YOLO models
â”œâ”€â”€ tests/                   # Test files
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Docker configuration
â”œâ”€â”€ docker-compose.yml      # Docker Compose setup
â””â”€â”€ README.md               # This file
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the root directory:

```env
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Model Configuration
OPENCLIP_MODEL=ViT-B-32
OPENCLIP_PRETRAINED=openai
BLIP_MODEL=Salesforce/blip2-opt-2.7b

# Processing Configuration
MAX_FRAMES=1000
FRAME_SKIP=5
BATCH_SIZE=32

# Memory Management
MAX_MEMORY_GB=8
CACHE_SIZE=1000
```

### Model Configuration

Modify `src/utils/config.py` to adjust:
- Model selection and parameters
- Processing thresholds
- Memory limits
- Cache settings

## ğŸ¯ Use Cases

### Security & Surveillance
- Person tracking across multiple cameras
- Unusual activity detection
- Object recognition in security footage

### Content Analysis
- Video content moderation
- Scene understanding
- Object inventory from videos

### Research & Development
- Computer vision research
- Multi-modal AI experiments
- Video understanding benchmarks

## ğŸ”§ Advanced Usage

### Custom Model Integration

```python
from src.models.base_model import BaseModel

class CustomModel(BaseModel):
    def __init__(self):
        super().__init__()
        # Initialize your model
    
    def process(self, frames):
        # Your processing logic
        return results
```

### Pipeline Customization

```python
from src.pipeline.base_pipeline import BasePipeline

class CustomPipeline(BasePipeline):
    def process_video(self, video_path, query):
        # Custom processing pipeline
        return results
```

## ğŸ“Š Performance

### Benchmarks
- **Processing Speed**: ~30 FPS on GPU, ~5 FPS on CPU
- **Memory Usage**: 2-8GB depending on models loaded
- **Accuracy**: 85-95% depending on query complexity

### Optimization Tips
- Use GPU acceleration when available
- Adjust batch size based on available memory
- Enable model caching for repeated queries
- Use frame skipping for faster processing

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup

```bash
# Install development dependencies
pip install -r requirements.txt

# Run tests
python -m pytest tests/

# Run linting
flake8 src/
black src/
```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [OpenCLIP](https://github.com/mlfoundations/open_clip) for vision-language models
- [BLIP-2](https://github.com/salesforce/BLIP) for image captioning
- [MediaPipe](https://mediapipe.dev/) for real-time perception
- [Ultralytics](https://ultralytics.com/) for YOLOv8
- [Streamlit](https://streamlit.io/) for the web interface
- [FastAPI](https://fastapi.tiangolo.com/) for the API framework

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/nb-hmd/Advanced-Video-Event-Detection-Extraction/issues)
- **Discussions**: [GitHub Discussions](https://github.com/nb-hmd/Advanced-Video-Event-Detection-Extraction/discussions)
- **Documentation**: [Wiki](https://github.com/nb-hmd/Advanced-Video-Event-Detection-Extraction/wiki)

## ğŸ”® Roadmap

- [ ] Real-time video streaming support
- [ ] Mobile app development
- [ ] Cloud deployment templates
- [ ] Additional model integrations
- [ ] Performance optimizations
- [ ] Multi-language support

---

**Made with â¤ï¸ for the computer vision community**